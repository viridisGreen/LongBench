{"pred": "\n\nAnswer: The ground truth for fake news is established by the authors of the dataset.\n\nAnswer: The ground truth for fake news is established by the authors of the dataset.\n\nAnswer: The ground truth for fake news is established by the authors of the dataset.\n\nAnswer: The ground truth for fake news is established by the authors of the dataset.\n\nAnswer: The ground truth for fake news is established by the authors of the dataset.\n\nAnswer: The ground truth for fake news is established by the authors of the dataset.\n\nAnswer: The ground truth for fake news is established by the", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "GhostVLAD is an extension of the NetVLAD approach, which we discussed in the previous section. The GhostVLAR works exactly similar to NetVLAR except it adds Ghost clusters along with the NetVLAR clusters. So, now we will have a K+G number of clusters instead of K clusters. The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are not included during the feature aggregation stage, as shown in Figure 1 (Right side).\n\nQuestion: What is the ResNet?\n\nAnswer:[/INST] The ResNet is used", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "\nBy how much does their model outperform the state of the art results?\n\nAnswer: By how much does their model outperform the state of the art results?\n\nAnswer: By how much does their model outperform the state of the art results?\n\nAnswer: By how much does their model outperform the state of the art results?\n\nAnswer: By how much does their model outperform the state of the art results?\n\nAnswer: By how much does their model outperform the state of the art results?\n\nAnswer: By how much does their model outperform the state of the art", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "\n\nAnswer:\n\n\\section{Empirical Results}\n\n\\section{Traditional Machine Learning Models}\n\n\\section{Neural Network Models}\n\n\\section{Ensemble Models}\n\n\\section{Discussion and Conclusion}\n\n\\section{Acknowledgments}\n\n\\section{References}\n\n[1] Founta, A., Parabasem, A., Graff, M., and Igel, B. I. (2018). Hate and Abusive Speech on Twitter: ", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "\n\n[INST]\n\nAnswer:\n\n[/INST]\n\n[INST]\n\nAnswer:\n\n[/INST]\n\n[INST]\n\nAnswer:\n\n[/INST]\n\n[INST]\n\nAnswer:\n\n[/INST]\n\n[INST]\n\nAnswer:\n\n[/INST]\n\n[INST]\n\nAnswer:\n\n[/INST]\n\n[INST]\n\nAnswer:\n\n[/INST]\n\n[INST]\n\nAnswer:\n\n[/INST]\n\n[INST]\n\nAnswer", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "No. The hashtag dataset contains 1,108 hashtags in 1,108 tweets in 10 languages. The SemEval dataset contains 12,594 hashtags in 12,594 tweets in 10 languages.\n\nQuestion: What is the dataset split?\n\nAnswer:[/INST]The hashtag dataset is split into 1,108 hashtags in 1,108 tweets in 10 languages. The SemEval dataset is split into 12,594", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "unanswerable\n\n Question: What is the purpose of the concept-map-based MDS task?\n\nAnswer:[/INST]introduce a new task\n\n Question: What is the purpose of the low-context importance annotation?\n\nAnswer:[/INST]crowdsourcing\n\n Question: What is the purpose of the corpus creation?\n\nAnswer:[/INST]create a corpus\n\n Question: What is the purpose of the corpus analysis?\n\nAnswer:[/INST]analyze the corpus\n\n Question: What is the purpose of the baseline experiments?", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "\nCNN/DailyMail, NYT, and XSum.\n\nQuestion: What are the datasets used for evaluation?\n\nAnswer:[/INST]\nCNN/DailyMail, NYT, and XSum.\n\nQuestion: What are the datasets used for evaluation?\n\nAnswer:[/INST]\nCNN/DailyMail, NYT, and XSum.\n\nQuestion: What are the datasets used for evaluation?\n\nAnswer:[/INST]\nCNN/DailyMail, NYT, and XSum.\n\nQuestion: What are the datasets used for evaluation", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "\n\nAnswer:\n\n\\section{Multi-sense word embeddings with Gaussian Mixtures}\n\n\\section{Introduction}\n\n\\section{Representing words as Gaussian Mixtures}\n\n\\section{Energy Function}\n\n\\section{Approximation for KL divergence between mixtures of gaussians}\n\n\\section{Strictly-stricter KL approximation}\n\n\\section{Conclusion}\n\n\\section{Appendices}\n\n\\section{Approximation for KL divergence between mixt", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "\n\n\\section{Introduction}\n\nThe task we are trying to solve consists of answering a cloze-style question, the answer to which depends on the understanding of a context document provided with the question. The model is also provided with a set of possible answers from which the correct one is to be selected. This can be formalized as follows:\n\nThe training data consist of tuples INLINEFORM0 , where INLINEFORM1 is a question, INLINEFORM2 is a document that contains the answer to question INLINEFORM3 , INLINEFORM4 is a set of possible answers and INLINEFORM5", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "\nThe datasets are composed of two subsets, Friends and EmotionPush, according to the needs for considering context at the same time. The former comes from the scripts of the Friends TV sitcom. The other is made up of Facebook messenger chats. Each subset includes $1,000$ English dialogues, and each dialogue can be further divided into a few consecutive utterances. All the utterances are annotated by five annotators on a crowd-sourcing platform (Amazon Mechanical Turk), and the labeling work is only based on the textual content. Annotator votes", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "\n\n English\n\nAnswer:[/INST]\n\n Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nArticle: Introduction\nText simplification aims to reduce the lexical and structural complexity of a text, while still retaining the semantic meaning, which can help children, non-native speakers, and people with", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "IMDb dataset.\n\nQuestion: What is the NER dataset?\n\nAnswer:[/INST]Groningen Meaning Bank.\n\nQuestion: What is the BW dataset?\n\nAnswer:[/INST]Billion Word.\n\nQuestion: What is the SW dataset?\n\nAnswer:[/INST]Simple Wiki.\n\nQuestion: What is the GMB dataset?\n\nAnswer:[/INST]Groningen Meaning Bank.\n\nQuestion: What is the IMDb dataset?\n\nAnswer:[/INST]IMDb dataset.\n\nQuestion: What is the BW", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "\nThe proposed system achieves 90.00\\% F1-F1 scores on the DL-PS dataset, 89.00\\% F1-F1 scores on the EC-MT dataset, and 90.00\\% F1-F1 scores on the EC-UQ dataset.\n\nQuestion: What is the impact of the pretrained character embeddings?\n\nAnswer:[/INST]\nThe pretrained embeddings have a significant impact on the performance of the proposed system. The comparison results are shown in Figure 2, where Random refers to", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes\n\nAnswer:[/INST]No\n\nAnswer:[/INST]Unanswerable\n\nAnswer:[/INST]Unanswerable\n\nAnswer:[/INST]Unanswerable\n\nAnswer:[/INST]Unanswerable\n\nAnswer:[/INST]Unanswerable\n\nAnswer:[/INST]Unanswerable\n\nAnswer:[/INST]Unanswerable\n\nAnswer:[/INST]Unanswerable\n\nAnswer:[/INST]Unanswerable\n\nAnswer:[/INST]Unanswerable\n\nAnswer:[/INST]Unanswerable\n\nAnswer:[/INST]Unanswer", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "\n\n[INST]The article is based on a dataset that is not publicly available.\n\n[INST]The article is based on a dataset that is not publicly available.\n\n[INST]The article is based on a dataset that is not publicly available.\n\n[INST]The article is based on a dataset that is not publicly available.\n\n[INST]The article is based on a dataset that is not publicly available.\n\n[INST]The article is based on a dataset that is not publicly available.\n\n[INST]The article is based on a dataset that is", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "\n\n\\section{Supporting Information}\n\n\\subsection{Global Model Approach}\n\n\\subsubsection{Multi-Stock Mini-Batch}\n\n\\begin{figure}[!h]\n\\centering\n\\includegraphics[width=0.45\\textwidth]{multi_stock_mini_batch.png}\n\\caption{Multi-Stock Mini-Batch}\n\\label{fig:multi_stock_mini_batch}\n\\end{figure}\n\n\\subsubsection{Stock Embedding}\n\n\\begin{figure}[!h]\n\\centering", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "\n\n RNN-based NMT. The basic RNN-based NMT model is based on BIBREF0 which is introduced above. Both the encoder and decoder used 2-layer RNN with 1024 LSTM cells, and the encoder is a bi-directional RNN. The batch size, threshold of element-wise gradient clipping and initial learning rate of Adam optimizer BIBREF21 were set to 128, 5.0 and 0.001. When trained the model on augmented dataset, we used 4-layer", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "\n\nThe three regularization terms are neutral features, the maximum entropy of class distribution regularization, and the KL divergence between reference and predicted class distribution.\n\nAnswer:\n\nThe three regularization terms are neutral features, the maximum entropy of class distribution regularization, and the KL divergence between reference and predicted class distribution.\n\nAnswer:\n\nThe three regularization terms are neutral features, the maximum entropy of class distribution regularization, and the KL divergence between reference and predicted class distribution.\n\nAnswer:\n\nThe three regularization terms are neutral features, the maximum entropy of class distribution", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "\n\nThe baselines are SVM, CNN, and RCNN.\n\nQuestion: What is the FBFans dataset?\n\nAnswer:[/INST]\n\nThe FBFans dataset is a dataset of Facebook posts.\n\nQuestion: What is the FBFans dataset?\n\nAnswer:[/INST]\n\nThe FBFans dataset is a dataset of Facebook posts.\n\nQuestion: What is the FBFans dataset?\n\nAnswer:[/INST]\n\nThe FBFans dataset is a dataset of Facebook posts.\n\nQuestion: What is the FBFans dataset?\n\n", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "\n\n\\section{Introduction}\n\nAutomatic classification of sentiment has mainly focused on categorizing tweets in binary or ternary categories. In this work we study the problem of fine-grained sentiment classification where tweets are classified according to a five-point scale ranging from VeryNegative to VeryPositive. To illustrate this, Table TABREF1 presents examples of tweets associated with each of these categories. Five-point scales are widely adopted in review sites like Amazon and TripAdvisor, where a user's sentiment is ordered with respect to its intensity. From a sentiment analysis perspective,", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "\nThe authors of the paper introduce a new model that allows for more precise interpretability of the attention mechanism. The authors argue that the attention mechanism is a complex system that can be understood by looking at the different ways in which the model's attention heads are able to look at different relationships between the input tokens.\n\nThe authors of the paper argue that the attention mechanism is a complex system that can be understood by looking at the different ways in which the model's attention heads are able to look at different relationships between the input tokens.\n\nThe authors of the paper argue that the attention mechanism is a complex system that can be", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "The baseline is the sentence-level MT system.\n\nQuestion: what is the model?\n\nAnswer:[/INST]The model is the monolingual DocRepair model.\n\nQuestion: what is the training data?\n\nAnswer:[/INST]The training data is the monolingual data.\n\nQuestion: what is the model?\n\nAnswer:[/INST]The model is the monolingual DocRepair model.\n\nQuestion: what is the training data?\n\nAnswer:[/INST]The training data is the monololingual data.\n", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "Unanswerable.\n\nQuestion: What is the impact of the model on the zero-shot performance?\n\nAnswer:[/INST]Unanswerable.\n\nQuestion: What is the impact of the model on the supervised performance?\n\nAnswer:[/INST]Unanswerable.\n\nQuestion: What is the impact of the model on the transfer performance?\n\nAnswer:[/INST]Unanswerable.\n\nQuestion: What is the impact of the model on the transfer performance?\n\nAnswer:[/INST]Unanswerable.\n\nQuestion: What is the impact of the model on the", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "The attention module is pre-trained on the MT dataset.\n\nQuestion: What is the role of the text encoder?\n\nAnswer:[/INST] The text encoder is a text-to-text model that is pre-trained on the MT dataset.\n\nQuestion: What is the role of the speech encoder?\n\nAnswer:[/INST] The speech encoder is a speech-to-text model that is pre-trained on the ASR dataset.\n\nQuestion: What is the role of the target text decoder?\n\nAnswer:[/INST] The target", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "\nThe encoder has a wide context window, encoding the entire available context.\n\nQuestion: What is the main task?\n\nAnswer:[/INST]\nThe main task is to generate an inflected word form given its lemma and the context in which it occurs.\n\nQuestion: What is the auxiliary task?\n\nAnswer:[/INST]\nThe auxiliary task is to predict the MSD tag of the target form.\n\nQuestion: What is the multilingual approach?\n\nAnswer:[/INST]\nThe multilingual approach is a multilingual training of the auxiliary task", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes, it is.\n\nAnswer:[/INST]No, it is not.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "\n\n\\section{Acknowledgments}\n\nWe would like to thank the authors of the papers we used as baselines.\n\n\\section{References}\n\n[^1]: https://www.njhealth.gov/health-info-tools-and-data/health-data-and-data-tools/nj-health-statistics-data/nj-health-statistics-data-2019/nj-health-statistics-data-2019-nj-health-statistics-data", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "\n\nAnswer: 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "\n\nAnswer:\n\n\\section{Recipe-level Coherence}\n\nWe evaluate the quality of generated recipes by measuring the recipe-level coherence of each generated recipe. We use the neural scoring model from BIBREF33 to measure recipe-level coherence for each generated recipe. Each recipe step is encoded by BERT BIBREF34. Our scoring model is a GRU network that learns the overall recipe step ordering structure by minimizing the cosine similarity of recipe step hidden representations presented in the correct and reverse orders. Once pre", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "\n\n(1) The labels are created by the templates.\n\n(2) The labels are created by the templates.\n\n(3) The labels are created by the templates.\n\n(4) The labels are created by the templates.\n\n(5) The labels are created by the templates.\n\n(6) The labels are created by the templates.\n\n(7) The labels are created by the templates.\n\n(8) The labels are created by the templates.\n\n(9) The labels are created by the templates.\n\n(10) The labels are created by", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "\n\n\\section{Introduction}\n\nAssembling training corpora of annotated natural language examples in specialized domains such as biomedicine poses considerable challenges. Experts with the requisite domain knowledge to perform high-quality annotation tend to be expensive, while lay annotators may not have the necessary knowledge to provide high-quality annotations. A practical approach for collecting a sufficiently large corpus would be to use crowdsourcing platforms like Amazon Mechanical Turk (MTurk). However, crowd workers in general are likely to provide noisy annotations BIBREF0 , BIBREF1 , BIB", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "Unanswerable.\n\nQuestion: What is the Transformer architecture?\n\nAnswer:[/INST]The Transformer is a seq2seq model that uses hierarchical multi-head attention mechanisms to dynamically attend to different parts of the input.\n\nQuestion: What is the difference between the Transformer and other seq2seq models?\n\nAnswer:[/INST]The Transformer is a seq2seq model that uses hierarchical multi-head attention mechanisms to dynamically attend to different parts of the input.\n\nQuestion: What is the difference between the Transformer and other seq2seq models?\n", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "The improvement in performance for Estonian in the NER task is the largest for all languages.\n\nQuestion: What is the improvement in performance for Swedish in the NER task?\n\nAnswer:[/INST] The improvement in performance for Swedish in the NER task is the largest for all languages.\n\nQuestion: What is the improvement in performance for Lithuanian in the NER task?\n\nAnswer:[/INST] The improvement in performance for Lithuanian in the NER task is the largest for all languages.\n\nQuestion: What is the improvement in performance for Croatian in the NER task", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "No. The paper is an unsupervised approach to spam detection.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer:[/INST] The Honeypot dataset is a public dataset. The Weibo dataset is a self-collected dataset.\n\nQuestion: What is the difference between the two features?\n\nAnswer:[/INST] The two features are different. The first feature is the Local Outlier Standard Score. The second feature is the Global Outlier Standard Score.\n\nQuestion: What is the performance of the two features?\n\nAnswer:[/INST", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "\n\n\\section{Introduction}\n\n\\begin{blockquote}\n\n\\begin{blockquote}\n\n\\begin{blockquote}\n\n\\begin{blockquote}\n\n\\begin{blockquote}\n\n\\begin{blockquote}\n\n\\begin{blockquote}\n\n\\begin{blockquote}\n\n\\begin{blockquote}\n\n\\begin{blockquote}\n\n\\begin{blockquote}\n\n\\begin{blockquote}\n\n\\begin{blockquote}\n\n\\begin{blockquote}\n\n\\begin{blockquote}\n\n\\begin{blockquote}\n\n\\begin{blockquote}\n\n", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "Their data set is large.\n\nQuestion: What is the purpose of the paper?\n\nAnswer:[/INST]The paper's purpose is to answer the question.\n\nQuestion: What is the main idea of the paper?\n\nAnswer:[/INST]The main idea of the paper is that the answer is the answer.\n\nQuestion: What is the main point of the paper?\n\nAnswer:[/INST]The main point of the paper is that the answer is the answer.\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer:[/INST]The main conclusion", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes.\n\nQuestion: What is the main focus of the paper?\n\nAnswer:[/INST]The paper is about a unified approach to seamlessly extend the NMT to multilingual settings.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer:[/INST]The main contribution is the proposed unified approach to seamlessly extend the NMT to multilingual settings.\n\nQuestion: What is the state-of-the-art in the field?\n\nAnswer:[/INST]The state-of-the-art is the NMT framework.\n\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "\n\nAnswer: The authors evaluate the effectiveness of their approach by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews. They quantify the efficiency of a communication scheme by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The effectiveness of the approach is shown by the fact that the constrained objective is more stable than the linear objective and that the constrained model achieves a 52.16% improvement in accuracy at a 77.37% retention rate compared to Unif, which is", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "The source domain is the set of reviews from the book, DVDs, and electronics domains. The target domain is the set of reviews from the kitchen domain.\n\nQuestion: What is the labeled data in the source and target domains?\n\nAnswer:[/INST] The labeled data in the source and target domains are the reviews from the source and target domains.\n\nQuestion: What is the unlabeled data in the source and target domains?\n\nAnswer:[/INST] The unlabeled data in the source and target domains are the reviews from the unlabeled data in the source and target domains", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "\n\nThe authors compare the performance of the PRU with the LSTM and the GRU.\n\nQuestion: what is the difference between the LSTM and the PRU?\n\nAnswer:[/INST]\n\nThe LSTM is a standard RNN architecture that uses a gating structure to control the flow of information. The PRU is a new RNN architecture that uses a pyramidal transformation to enable higher dimensional vector representations.\n\nQuestion: what is the difference between the GRU and the PRU?\n\nAnswer:[/INST]\n\nThe GRU is a standard RNN", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "\n\nThe NeuronBlocks toolkit is a DNN toolkit for NLP tasks. The toolkit consists of two layers. The upper layer targets common NLP tasks. For each task, the toolkit provides several end-to-end network templates, which can be instantiated with simple configuration. The bottom layer consists of a suite of reusable and standard components, which can be adopted as building blocks to construct networks with complex architecture.\n\nThe toolkit is built on PyTorch. The overall framework is illustrated in Figure FIGURE-1. It consists of two layers: the Block Zoo and", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "\n\n[INST]\n\n\\section{Introduction}\n\n\\textbf{Cross-lingual transfer learning} aims to leverage large datasets in one language—usually English—to build multilingual models that can generalize to other languages. Previous work has explored 3 main approaches to that end: \\textbf{1)} \\textbf{Parallel data}: machine-to-human and human-to-human machine-assisted translations of the training set into each target language, followed by fine-tuning the resulting models in each language. \\textbf{2)} \\textbf{H", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "\n\n\\section{Bibliography}\n\n\\section{BIBREF0}\n\n\\section{BIBREF1}\n\n\\section{BIBREF2}\n\n\\section{BIBREF3}\n\n\\section{BIBREF4}\n\n\\section{BIBREF5}\n\n\\section{BIBREF6}\n\n\\section{BIBREF7}\n\n\\section{BIBREF8}\n\n\\section{BIBREF9}\n\n\\section{BIBREF10}\n\n\\section{BIBREF", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes.\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]The dataset is the WikiBio dataset.\n\nQuestion: What is the model?\n\nAnswer:[/INST]The model is a neural network with bifocal attention and gated orthogonalization.\n\nQuestion: What is the performance?\n\nAnswer:[/INST]The performance is 21% better than the state of the art model.\n\nQuestion: What is the improvement?\n\nAnswer:[/INST]The improvement is 21% better than the state of the art model.\n\nQuestion", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "No. The system is a demo system, and the performance of the system is not evaluated against any baseline.\n\nQuestion: What is the dataset used for training the model?\n\nAnswer:[/INST]The training data is a large set of conversational and image data from Reddit.\n\nQuestion: What is the dataset used for evaluation?\n\nAnswer:[/INST]The evaluation is not performed. The system is a demo system, and the performance of the system is not evaluated against any baseline.\n\nQuestion: What is the dataset used for testing?\n\nAnswer:[/INST]The dataset", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "\n\n### 2. 2016-04-10 10:00:00\n\n[INST]You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nArticle: Introduction\nBlogging gained momentum in 1999", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "\n\nAnswer: The ML methods aim to identify the argument components in the form of premises, rebuttals, and refutations.\n\nAnswer: The ML methods aim to identify the argument components in the form of premises, rebuttals, and refutations.\n\nAnswer: The ML methods aim to identify the argument components in the form of premises, rebuttals, and refutations.\n\nAnswer: The ML methods aim to identify the argument components in the form of premises, rebuttals, and refutations.\n\nAnswer: The ML methods aim", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "\n\n\\begin{code}\nPARENT: 1\n\\end{code}\n\n\\end{code}\n\n\\end{code}\n\n\\end{code}\n\n\\end{code}\n\n\\begin{code}\n\n\\begin{code}\n\\begin{table}[!h]\n\\centering\n\\begin{tabular}{l}\n\\textbf{Birth-date}: 2000-01-01 \\\\\n\\textbf{Birth-place}: United States \\\\\n\\textbf{Birth-country}: United States \\\\\n\\", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "Unanswerable.\n\nQuestion: What is the dataset used for the analysis?\n\nAnswer:[/INST]Twitter.\n\nQuestion: What is the dataset used for the analysis?\n\nAnswer:[/INST]Online support group.\n\nQuestion: What is the dataset used for the analysis?\n\nAnswer:[/INST]Twitter.\n\nQuestion: What is the dataset used for the analysis?\n\nAnswer:[/INST]Online support group.\n\nQuestion: What is the dataset used for the analysis?\n\nAnswer:[/INST]Twitter.\n\nQuestion: What", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "\n\n12 languages are covered in the Multi-SimLex dataset. The 12 languages are:\n\n\\begin{code}\nEnglish, French, Spanish, Portuguese, Italian, German, Dutch, Russian, Welsh, Kiswahili, Yue Chinese, and Mandarin.\n\\end{code}\n\nQuestion: What is the total number of concept pairs?\n\nAnswer:[/INST]\n\nThe total number of concept pairs is 1,888.\n\nQuestion: How is the dataset created?\n\nAnswer:[/INST]\n\nThe dataset is", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "\n\n Wikipedia Talk Page Conversions.\n ChangeMyView.\n\n Question: What is the main goal of the model?\n\n Answer:[/INST]\n\n To predict the future derailment of a conversation.\n\n Question: What is the main challenge in this task?\n\n Answer:[/INST]\n\n To capture the dynamics of a conversation as it develops.\n\n Question: What is the main contribution of the model?\n\n Answer:[/INST]\n\n A model that can capture the dynamics of a conversation as it develops.\n\n Question: What is the main limitation", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "No.\n\nQuestion: What is the main goal of the framework?\n\nAnswer:[/INST] The main goal of the framework is to extract information from texts and populate a knowledge base.\n\nQuestion: What is the main contribution of the work?\n\nAnswer:[/INST] The main contribution of the work is the creation of a full NLP pipeline for the Portuguese language.\n\nQuestion: What are the main improvements of the work?\n\nAnswer:[/INST] The main improvements of the work are the creation of a full NLP pipeline for the Portuguese language and the ontology matching.\n\nQuestion", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The quality of the data is evaluated by the BLEU score.\n\nQuestion: What is the purpose of the CoVoST corpus? \n\nAnswer:[/INST] The CoVoST corpus is a multilingual speech-to-text translation corpus.\n\nQuestion: What are the 11 languages? \n\nAnswer:[/INST] The 11 languages are French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian, and Chinese.\n\nQuestion: What is the purpose of the Tatoeba evaluation spl", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "\nThe authors propose a multimodal dual recurrent encoder model that simultaneously utilizes text data, as well as audio signals, to permit the better understanding of speech data. Their model encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model to predict the emotion class.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "\n\n\\section{Introduction}\n\n\\textbf{The problem}\n\nThe problem is to produce a more concise version of a given text.\n\n\\textbf{The state-of-the-art}\n\nThe state-of-the-art is a neural network model that is trained on a large corpus of text.\n\n\\textbf{The proposed method}\n\nThe proposed method is a neural network model that is trained on a large corpus of text.\n\n\\textbf{The results}\n\nThe results are that the proposed method is better than the state-of-the", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "unanswerable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "LSTM-CRF\n\nQuestion: What is the best way to combine multiple neural architectures?\n\nAnswer:[/INST]Ensemble\n\nQuestion: What is the best way to combine multiple neural architectures?\n\nAnswer:[/INST]Ensemble\n\nQuestion: What is the best way to combine multiple neural architectures?\n\nAnswer:[/INST]Ensemble\n\nQuestion: What is the best way to combine multiple neural architectures?\n\nAnswer:[/INST]Ensemble\n\nQuestion: What is the best way to combine multiple neural architectures?\n\n", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "\nThe data is collected by the DeepMine project.\n\nQuestion: what is the purpose of the data?\n\nAnswer:[/INST]\nThe data is collected for the purpose of text-based speaker verification.\n\nQuestion: what is the dataset?\n\nAnswer:[/INST]\nThe dataset is a large-scale dataset for text-based speaker verification.\n\nQuestion: what is the dataset size?\n\nAnswer:[/INST]\nThe dataset size is 1969 speakers with 1149 males and 820 females.\n\nQuestion: what", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "\n\n\\section{RQE-based QA}\n\nIn this section we present the RQE-based QA approach for the medical domain.\n\n\\subsection{RQE-based QA Approach}\n\nThe RQE-based QA approach is based on the following steps:\n\n\\begin{enumerate}\n\\item Candidate question retrieval:\n\\begin{itemize}\n\\item Candidate question retriection from the collection of 47K question-answer pairs.\n\\item Focus recognition:\n\\begin{itemize}\n\\", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "\nThe Honeypot dataset is a public dataset. It is a good dataset for spammer detection.\n\nQuestion: What is the reason for the performance of the two features?\n\nAnswer:[/INST]\nThe two features are extracted from the LDA model. The LDA model is a topic model. The two features are extracted from the LDA model. The LDA model is a topic model. The two features are extracted from the LDA model. The LDA model is a topic model. The two features are extracted from the LDA model. The LDA model is a topic model. The two", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "\nThe decoder has a character-based encoder-decoder architecture.\n\nQuestion: What is the main augmentation to the baseline system?\n\nAnswer:[/INST]\nThe main augmentation to the baseline system is a wide context window, encoding the entire available context.\n\nQuestion: What is the auxiliary objective?\n\nAnswer:[/INST]\nThe auxiliary objective is the auxiliary task of MSD prediction, which acts as a regulariser.\n\nQuestion: How does the multilingual approach help?\n\nAnswer:[/INST]\nThe multilingual approach helps", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes.\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]The FS Dataset (FSD) is a large-scale dataset for open-domain event extraction. It contains 1,000 tweets annotated with 20 events.\n\nGoogle dataset (news article) is a subset of GDELT Event Database.\n\nWe combine 30 events related documents, the dataset contains 11,909 news articles.\n\nAnswer:[/INST]The FS Dataset (FSD) is a large-scale dataset for open-domain event", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "\n\\begin{itemize}\n\\item \\textbf{MIC-CIS}: \\textbf{Team: MIC-CIS}\n\\item \\textbf{Sentence-level Propaganda}: \\textbf{Sentence-level Propaganda}\n\\item \\textbf{Fragment-level Propaganda}: \\textbf{Fragment-level Propaganda}\n\\end{itemize}\n\n\\section{Team: MIC-CIS}\n\\textbf{Sentence-level Propaganda}: \\textbf{Sentence-level Propaganda}\n\\textbf{Fragment", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "\nThe baseline is the M2M model.\n\nQuestion: what is the M2M model?\n\nAnswer:[/INST]\nThe M2M model is a multilingual model that is pre-trained on a large-scale out-of-domain parallel corpus.\n\nQuestion: what is the in-domain corpus?\n\nAnswer:[/INST]\nThe in-domain corp is the cleaned trilingual corp.\n\nQuestion: what is the cleaned trilingual corp?\n\nAnswer:[/INST]\nThe cleaned trilingual", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: What is the highest ‘MRR’ score for Factoid Question Answering task?\n\nAnswer:[/INST]\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "\n\nAnswer: The paper explores the use of distributional techniques to quantify semantic similarity and relatedness. These techniques are based on the distributional hypothesis, which holds that two concepts that are similar are also related.\n\nAnswer: The paper explores the use of distributional techniques to quantify semantic similarity and relatedness. These techniques are based on the distributional hypothesis, which holds that two concepts that are similar are also related.\n\nAnswer: The paper explores the use of distributional techniques to quantify semantic similarity and relatedness. These techniques are based on the distributional hypothesis, which holds that two concepts", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "\n\nThe CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-ordering rules: (1) generic rules that apply to all Indian languages and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "Yes\n\nQuestion: Does the paper discuss the challenges of information extraction in the biomedical domain?\n\nAnswer:[/INST]Yes\n\nQuestion: What are the main problems in BioIE?\n\nAnswer:[/INST]The main problems in BioIE are similar to those in Information Extraction:\n\n\\begin{code}\n1. The explosion of available biomedical text in the Biomedical domain has led to the rise of Biomedical Information Extraction (BioIE).\n2. BioIE systems aim to extract information from a wide spectrum of biomedical text including", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "\n\nThe following table presents the demographic information of the annotators.\n\nTable.TABREF1: Demographic Information of the Annotators\n\n\\begin{table}[!h]\n\\tabref{TABREF1}\n\\end{table}\n\nThe following table presents the demographic information of the crowdworkers.\n\nTable.TABREF2: Demographic Information of the Crowdworkers\n\n\\begin{table}[!h]\n\\tabref{TABREF2}\n\\end{table}\n\nThe following table presents the demographic information", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "\n\n### [BIBREF0]\n\n[BIBREF1]\n\n[BIBREF2]\n\n[BIBREF3]\n\n[BIBREF4]\n\n[BIBREF5]\n\n[BIBREF6]\n\n[BIBREF7]\n\n[BIBREF8]\n\n[BIBREF9]\n\n[BIBREF10]\n\n[BIBREF11]\n\n[BIBREF12]\n\n### [FIGREF1]\n\n### [FIGREF2]\n", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "\n\\textbf{unanswerable}\n\nAnswer:[/INST]\n\\textbf{unanswerable}\n\nAnswer:[/INST]\n\\textbf{unanswerable}\n\nAnswer:[/INST]\n\\textbf{unanswerable}\n\nAnswer:[/INST]\n\\textbf{unanswerable}\n\nAnswer:[/INST]\n\\textbf{unanswerable}\n\nAnswer:[/INST]\n\\textbf{unanswerable}\n\nAnswer:[/INST]\n\\textbf{unanswerable}\n\nAnswer:[/INST]\n\\textbf{unanswerable}", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "Yes.\n\nQuestion: What is the main reason for the gap between MRC models and human beings?\n\nAnswer:[/INST]The main reason for the gap is that MRC models can only utilize the knowledge contained in each given passage-question pair, but in addition to this, human beings can also utilize general knowledge.\n\nQuestion: What is the understanding of general knowledge?\n\nAnswer:[/INST]The understanding of general knowledge is that it is a set of inter-word semantic connections.\n\nQuestion: What is the understanding of the explicit way to utilize general knowledge?\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "\n\nAnswer:\n\nThe authors of the article discuss three topics of cyberbullying. The first topic is racism, which is defined as the use of the internet, cell phones or other devices to swear or threaten a person of a different race. The second topic is sexism, which is defined as the use of the internet, cell phones or other devices to swear or threaten a woman. The third topic is personal attack, which is defined as the use of the internet, cell phones or other devices to swear or threaten a person.\n\nQuestion: What vocabulary words did", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "\nExtended Middle Context\nThe input is a sentence. The sentence is split into three disjoint regions based on the two arguments. The first region is the left context, the second is the middle context and the third is the right context.\n\nThe middle context is split into two parts: a combination of the left context, the left argument and the middle context and a combination of the middle context, the right argument and the right context.\n\nThe two contexts are processed by two convolutional and max-pooling layers. After pooling, the results are concatenated to form the sentence representation.\n\nFigure", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "Unanswerable\n\nQuestion: What is the maximum number of entities in a single sentence?\n\nAnswer:[/INST]Unanswerable\n\nQuestion: What is the maximum number of entities in a single sentence?\n\nAnswer:[/INST]Unanswerable\n\nQuestion: What is the maximum number of entities in a single sentence?\n\nAnswer:[/INST]Unanswerable\n\nQuestion: What is the maximum number of entities in a single sentence?\n\nAnswer:[/INST]Unanswerable\n\nQuestion: What is the maximum number of entities in a single sentence?\n\nAnswer:[/", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "\n\nAnswer: The annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer:[/INST]\n\nAnswer: The annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer:[/INST]\n\nAnswer: The annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer:[/INST]\n\nAnswer: The annotated data is of higher quality.\n\nQuestion: How much higher quality is the", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "\n\nThe main experiments are based on the 2016 and 2018 test sets for French and German.\n\nQuestion: What is the main difference between the base and the enriched models?\n\nAnswer:[/INST]\n\nThe enriched models are multimodal, combining text and image information.\n\nQuestion: What is the main improvement of the enriched models?\n\nAnswer:[/INST]\n\nThe enriched models show a more positive outlook towards the addition of visual information.\n\nQuestion: What is the main reason for the improvement of", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: What are the different types of decoders?\n\nAnswer:[/INST]\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article,", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "\nAnswer:\n\n### Q1. What is the main topic of the article?\n\n### Q2. What is the main goal of the article?\n\n### Q3. What is the cost-effectiveness of using crowdsourcing for obtaining new keywords and consequently, their expectations?\n\n### Q4. What is the effectiveness of our expectation inference method?\n\n### Q5. What is the main topic of the article?\n\n### Q6. What is the main goal of the article?\n\n### Q7. What is the", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "\n\n\\begin{code}\n<p>\n<a href=\"https://www.google.com/crowdpower/crowdreviews/google-cloud-text-analytics\">Google Cloud</a>\n<a href=\"https://www.tensitrst.com/\">TensiStrength</a>\n<a href=\"https://www.rosetta.com/\">Rosette Text Analytics</a>\n<a href=\"https://www.twittern.com/\">TwitterNLP</a>\n<a href=\"https", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "\n\nSQuAD 2.0, SQuAD 1.5, SQuAD 1.1, SQuAD 1.0, SQuAD 0.5, SQuAD 0.3, SQuAD 0.2, SQuAD 0.1, SQuAD 0.01, SQuAD 0.001, SQuAD 0.0001, SQuAD 0.00001, SQuAD 0.000001, SQuAD 0.0", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "\n\nThe use of Flickr for modelling urban environments has already received considerable attention. For instance, various approaches have been proposed for modelling urban regions BIBREF0 , and for identifying points-of-interest BIBREF1 and itineraries BIBREF2 , BIBREF3 . However, the usefulness of Flickr for characterizing the natural environment, which is the focus of this paper, is less well-understood.\n\nMany recent studies have highlighted that Flickr tags capture valuable ecological information, which can be used as a complementary source to more traditional", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "\n\n### Suggested solution\n\n```python\n# -*- coding: utf-8 -*-\n\n# import the necessary packages\n# ---------------------------------\nfrom __future__ import annotations\nimport pickle\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.backprop\nimport torch.nn.functional as F\nimport torch.optimization as optimizer\nimport torch.utils.data as data_utils\nimport torch.nn.functional as F\nimport", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "Fisher Phase 1 US English corpus is often used for automatic speech recognition in speech community. In this work, we used it for topic identification as in BIBREF3. The documents are 10-minute long telephone conversations between two people discussing a given topic. We used same training and test splits as BIBREF3 in which 1374 and 1372 documents are used for training and testing respectively. For validation of our model, we used 10% of training dataset and the remaining 90% was used for actual model training. The number of", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "\nSentiment classification dataset is used.\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]\nThe dataset is PUBLICATIONREF1 .\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]\nThe dataset is PUBLICATIONREF2 .\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]\nThe dataset is PUBLICATIONREF3 .\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]\nThe dataset is PUBLICATIONREF4 .\n\nQuestion: What is the dataset?\n\n", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "\n\n\\begin{code}\nAnswer:\n\n\\textbf{Answer}:\n\n\\textbf{Unanswerable}:\n\n\\textbf{No}:\n\n\\textbf{Yes}:\n\\end{code}\n\n\\end{code}\n\n\\begin{code}\n\n\\textbf{Unanswerable}:\n\n\\textbf{No}:\n\n\\textbf{Yes}:\n\\end{code}\n\n\\end{code}\n\n\\begin{code}\n\n\\textbf{Unanswerable}:\n\n\\textbf{No}:\n\n\\textbf{Yes}:\n\\", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "\nInvertibility condition is the condition that the projection is an invertible function.\n\nQuestion: What is the purpose of the projection?\n\nAnswer:[/INST]\nThe purpose of the projection is to transform the pre-trained word embeddings into a new representation space that is more suitable for the structured syntax model.\n\nQuestion: What is the Gaussian HMM?\n\nAnswer:[/INST]\nGaussian HMM is a locally normalized model with multinomial transitions and Gaussian emissions.\n\nQuestion: What is the Gaussian emissions?\n\nAnswer:[/INST]", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "\n\n\\section{Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nArticle: Introduction\nText simplification aims to reduce the lexical and structural complexity of a text, while still retaining the semantic meaning, which can help children, non-native speakers, and people with cognitive disabilities, to understand", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "\nThe baselines are the previous methods that are used as a benchmark for the proposed model.\n\nQuestion: What are the problems with the previous methods?\n\nAnswer:[/INST]\nThe previous methods have the following problems:\n\n1. The sub-nets are not reused in the pre-training and fine-tuning stages.\n2. The roles of the sub-nets are different in the pre-training and fine-tuning stages.\n3. The MT attention module is not used in the ST stage.\n\nQuestion: What is the Tandem Connectionist Encoding Network", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English\n\nQuestion: What is the main focus of the paper?\n\nAnswer:[/INST]The challenges of imbalanced classification\n\nQuestion: What is the main focus of the paper?\n\nAnswer:[/INST]The challenges of imbalanced classification\n\nQuestion: What is the main focus of the paper?\n\nAnswer:[/INST]The challenges of imbalanced classification\n\nQuestion: What is the main focus of the paper?\n\nAnswer:[/INST]The challenges of imbalanced classification\n\nQuestion: What is the main focus of the paper?\n\n", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "\n\n\\section{References}\n\n1.  BIBREF0 : Wase, M., et al. (2017). A study on the perception of offensive content in social media. In Proceedings of the 2017 ACM Workshop on the Social Implications of AI, 111–116.\n\n2.  BIBREF1 : Xu, X., et al. (2016). Aggressive comment classification in Twitter. In Proceedings of the 2016 ACM Workshop on the", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "\n\n\\begin{code}\n\\begin{document}\n\\begin{inst}{Question}\n\\begin{tabular}{p}\n\\textbf{Question:} & \\textbf{Answer:}\n\\\\[0.5em]\n\\end{tabular}\n\\end{inst}\n\\end{document}\n\\end{code}\n\n\\end{code}\n\n\\end{code}\n\n\\end{code}\n\n\\end{code}\n\n\\end{code}\n\n\\end{code}\n\n\\end{code}\n\n\\end{code", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "\nThe combination of rewards for reinforcement learning is a set of rewards that are used to guide the behavior of an agent in a reinforcement learning environment. The rewards can be positive or negative, and they are typically based on the performance of the agent in the environment.\n\n### Reward-based RL\n\nReward-based RL is a type of reinforcement learning that is based on the concept of rewards. Reward-based RL is a type of reinforcement learning that is based on the concept of rewards. Reward-based R", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "\n\n### [BIBREF0]\n\n[BIBREF1]\n\n[BIBREF2]\n\n[BIBREF3]\n\n[BIBREF4]\n\n[BIBREF5]\n\n[BIBREF6]\n\n[BIBREF7]\n\n[BIBREF8]\n\n[BIBREF9]\n\n[BIBREF10]\n\n[BIBREF11]\n\n[BIBREF12]\n\n### [FIGREF1]\n\n### [FIGREF2]\n", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "\n\n\\section{Introduction}\n\nThis work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/\n\nIn the spirit of the brevity of social media's messages and reactions, people have got used to express feelings minimally and symbolically, as with hashtags on Twitter and Instagram. On Facebook, people tend to be more wordy, but posts normally receive more simple “likes” than longer comments. Since February 2016, Facebook users can", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "\n\nAnswer: The distribution of friends and followers of accounts that produce viral tweets with fake news is different than the distribution of friends and followers of accounts that produce viral tweets without fake news.\n\nQuestion: What were their density representations?\n\nAnswer:[/INST]\n\nAnswer: The density representation of friends and followers of accounts that produce viral tweets with fake news is different than the density representation of friends and followers of accounts that produce viral tweets without fake news.\n\nQuestion: What were their representations of friends and followers?\n\nAnswer:[/INST]", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The hashtag dataset is sourced from the Twitter public archive.\n\nQuestion: What is the size of the hashtag dataset?\n\nAnswer:[/INST]The has size of the hashtag dataset is 12,594 hashtags.\n\nQuestion: What is the size of the dataset for the pairwise neural ranking model?\n\nAnswer:[/INST]The pairwise neural ranking model is trained on 1,108 hashtags.\n\nQuestion: What is the dataset for the sentiment analysis task?\n\nAnswer:[/INST]The sentiment analysis task", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "\n\n\\section{Introduction}\n\nIn this paper, we propose a new method for text classification, based on the novel concept of word subspace under the MSM framework. We also propose the term-frequency weighted word subspace which can incorporate the frequency of words directly in the modeling of the subspace by using a weighted version of PCA.\n\nMost of the conventional text classification methods are based on the bag-of-words features, which are very simple to compute and had been proved to produce positive results. However, bag-of-words are commonly high dimensional models, with a", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "The baseline model is a linear SVM model.\n\nQuestion: What is the performance of the baseline model?\n\nAnswer:[/INST]The performance of the baseline model is 0.12.\n\nQuestion: What is the performance of the proposed model?\n\nAnswer:[/INST]The performance of the proposed model is 0.66.\n\nQuestion: What is the performance of the baseline model for the year 2010?\n\nAnswer:[/INST]The performance of the baseline model for the year 2010 is 0.1", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "Yes.\n\nQuestion: What is the most common sense of the word \"research\"?\n\nAnswer:[/INST]The most common sense of the word \"research\" is \"the work involved in the experimental development of the principles of science, in the practical application of those principles, and in the invention of devices or processes that (1) require the principles of experimental science, and (2) are of the nature of industrial applications of the principles of experimental science.\"\n\nQuestion: What is the most common sense of the word \"research\"?\n\nAnswer:[/INST]The most common sense of the", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "\nThe Augmented LibriSpeech dataset is a large-scale dataset for training and evaluating speech recognition and speech translation models. It is a combination of the LibriSpeech dataset and the Common Voice dataset. The LibriSpeech dataset is a large-scale dataset for training and evaluating speech recognition models. The Common Voice dataset is a large-scale dataset for training and evaluating speech translation models.\n\nThe Augmented LibriSpeech dataset is a large-scale dataset for training and evaluating speech recognition and speech translation models. It is a combination of the LibriSpeech dataset and", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "\n\n\\section{Introduction}\n\nAutomatic classification of sentiment has mainly focused on categorizing tweets in binary or ternary categories. In this work we study the problem of fine-grained sentiment classification where tweets are classified according to a five-point scale ranging from VeryNegative to VeryPositive. To illustrate this, Table TABREF1 presents examples of tweets associated with each of these categories. Five-point scales are widely adopted in review sites like Amazon and TripAdvisor, where a user's sentiment is ordered with respect to its intensity. From a sentiment analysis perspective,", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "Large BERT.\n\nQuestion: What is the pre-trained BERT model?\n\nAnswer:[/INST]The pre-trained BERT model is the pre-trained BERT model.\n\nQuestion: What is the pre-trained BERT model?\n\nAnswer:[/INST]The pre-trained BERT model is the pre-trained BERT model.\n\nQuestion: What is the pre-trained BERT model?\n\nAnswer:[/INST]The pre-trained BERT model is the pre-trained BERT model.\n\nQuestion", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "Unanswerable.\n\nAnswer:[/INST]No.\n\nAnswer:[/INST]Yes.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "INLINEFORM0\n\nQuestion: What is the tagging scheme employed?\n\nAnswer:[/INST]INLINEFORM1\n\nQuestion: What is the tagging scheme employed?\n\nAnswer:[/INST]INLINEFORM2\n\nQuestion: What is the tagging scheme employed?\n\nAnswer:[/INST]INLINEFORM3\n\nQuestion: What is the tagging scheme employed?\n\nAnswer:[/INST]INLINEFORM4\n\nQuestion: What is the tagging scheme employed?\n\nAnswer:[/INST]INLINEFORM5\n\nQuestion: What is the tagging", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "No.\n\n### 2. 2019-06-12 CoVoST\n\n### 3. 2019-06-12 TT\n\n### 4. 2019-06-12 TT evaluation\n\n### 5. 2019-06-12 TT evaluation\n\n### 6. 2019-06-12 TT evaluation\n\n### 7. 2019-06-12 TT evaluation", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "\nRobustness is defined as the ability to handle bias in the prior knowledge.\n\nAnswer:\nRobustness is defined as the ability to handle bias in the prior knowledge.\n\nAnswer:\nRobustness is defined as the ability to handle bias in the prior knowledge.\n\nAnswer:\nRobustness is defined as the ability to handle bias in the prior knowledge.\n\nAnswer:\nRobustness is defined as the ability to handle bias in the prior knowledge.\n\nAnswer:\nRobustness is defined as the ability to handle bias in the prior knowledge.\n\nAnswer:\nRob", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "\n\n\\section{Sentence-BERT}\n\n\\section{Introduction}\n\nWe presented Sentence-BERT (SBERT), a modification of the BERT network using siamese and triplet networks that is able to derive semantic similarities. This enables BERT to be used for certain new tasks, which up-to-now were not applicable for BERT. These tasks include large-scale semantic similarity comparison, clustering, and information retrieval via semantic search.\n\nBERT set new state-of-the-art performance on various sentence classification and sentence-pair regression tasks. B", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "\n\n\\section{Introduction}\n\n\\textbf{Data Imbalance Issue in NLP}\n\nThe data imbalance issue is a common problem in NLP tasks. The majority of the datasets are composed of a large number of easy-positive examples and a small number of hard-negative examples. The easy-positive examples are the majority of the dataset, and the hard-negative examples are the minority of the dataset. The data imbalance issue is a severe problem for NLP tasks.\n\n\\textbf{Data Imbalance Issue in NLP ::: Binary Classification}\n\nThe", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "\n\\textbf{BIBREF0} RNN\n\\textbf{BIBREF1} LSTM\n\\textbf{BIBREF2} Gated RNT\n\\textbf{BIBREF3} CNN\n\\textbf{BIBREF4} RNTN\n\\textbf{BIBREF5} TA-LSTM\n\\textbf{BIBREF6} GRNT\n\\textbf{BIBREF7} RS\n\\textbf{BIBREF8} NI\n\\textbf{BIBREF9} TG-LSTM\n\\textbf{BIBREF", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "\n\n<p>\n\\textbf{Answer}: The core component for KBQA is the KBQA system.\n</p>\n\nQuestion: What is the core component for KBQA?\n\nAnswer:[/INST]\n\n<p>\n\\textbf{Answer}: The core component for KBQA is the KBQA system.\n</p>\n\nQuestion: What is the core component for KBQA?\n\nAnswer:[/INST]\n\n<p>\n\\textbf{Answer}: The core component for KBQA is the K", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "Baseline models are the encoder-decoder model and the hierarchical model.\n\nQuestion: What are the personalized models?\n\nAnswer:[/INST]Personalized models are the Prior Tech, Prior Name, and Prior Recipe models.\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]The dataset is a large-scale dataset of 180K+ recipes and 700K+ user reviews for the task of generating personalized recipes from incomplete input specifications.\n\nQuestion: What are the input specifications?\n\nAnswer:[/", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "\n\n\\section{Introduction}\n\nThe Flickr30K dataset BIBREF0 is a collection of over 30,000 images with 5 crowdsourced descriptions each. It is commonly used to train and evaluate neural network models that generate image descriptions (e.g. BIBREF2 ). An untested assumption overlooks the fact that the descriptions are based on the images, and nothing else. Here are the authors (about the Flickr8K dataset, a subset of Flickr30K):\n“By asking people to describe the people", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "\n\n```\n\"The girls\" is ambiguous.\n```\n\nAnswer:[/INST]\n\n```\n\"The girls\" is ambiguous.\n```\n\nAnswer:[/INST]\n\n```\n\"The girls\" is ambiguous.\n```\n\nAnswer:[/INST]\n\n```\n\"The girls\" is ambiguous.\n```\n\nAnswer:[/INST]\n\n```\n\"The girls\" is ambiguous.\n```\n\nAnswer:[/INST]\n\n```\n\"The girls\" is ambiguous.\n```\n\nAnswer:[/INST]\n\n", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "\n\nThe authors experimented with various models, including stacked LSTMs, LSTMs with peephole connections, and GRU.\n\nQuestion: What is the stacked LSTM model?\n\nAnswer:[/INST]\n\nThe stacked LAS model is a stacked LSTM model that uses the forget gate to control the vertical information flow.\n\nQuestion: What is the LSTM model?\n\nAnswer:[/INST]\n\nThe LSTM model is a stacked LSTM model that uses the forget gate to control the vertical information flow.", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "Yes.\n\nAnswer:[/INST]No.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]Unanswerable.\n\nAnswer:[/INST]", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "\n\nThe authors experimented with several summarization algorithms. The most important of these are:\n\n1. Sentence-based summarization: This type of summarization involves selecting a subset of sentences from the original text and then using these sentences to create a summary. The authors used the Sumy package to experiment with this type of summarization.\n\n2. Phrase-based summarization: This type of summarization involves selecting a subset of phrases from the original text and then using these phrases to create a summary. The authors used the Sumy package to experiment with this type of summarization.\n\n", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "\n\n\\section{Acknowledgements}\n\nWe thank the Coursera team for their help in obtaining the data. We also thank the reviewers for their insightful feedback.\n\n\\section{References}\n\n[1] BIBREF0 : B. Zhe, A. K. Iyrani, and A. C. Jadha, Predicting Instructor Intervention in Online Forums: A Probabilistic Graphical Model Approach. In Proceedings of the 2015 ACM Southeast Regional Conference, ACM S", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "Unanswerable\n\n### SECREF1. Introduction\n\nThe concept of message passing over graphs has been around for many years BIBREF0, BIBREF1, as well as that of graph neural networks (GNNs) BIBREF2, BIBREF3. However, GNNs have only recently started to be closely investigated, following the advent of deep learning. Some notable examples include BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "\n\nAnswer: The corpus used for the task is the diachronic corpus pair from BIBREF0.\n\nAnswer: The corpus used for the task is the diachronic corpus pair from BIBREF0.\n\nAnswer: The corpus used for the task is the diachronic corpus pair from BIBREF0.\n\nAnswer: The corpus used for the task is the diachronic corpus pair from BIBREF0.\n\nAnswer: The corpus used for the task is the diachronic corpus pair from BIBREF0.\n\nAnswer", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Kannada, Hindi, Telugu, Malayalam, Bengali, and English.\n\nQuestion: What is the dataset collection process?\n\nAnswer:[/INST]Crawled data from 2010 to 2019.\n\nQuestion: What is the amount of audio data for training and testing for each of the language?\n\nAnswer:[/INST]Kannada, Hindi, Telugu, Malayalam, Bengali, and English.\n\nQuestion: What is the ResNet architecture?\n\nAnswer:[/INST]ResNet-", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "\n\n\\section{Suggestion}\n\n\\section{Suggestion ::: Introduction}\n\n\\section{Suggestion ::: Experimental Setup and Data}\n\n\\section{Suggestion ::: Experimental Results}\n\n\\section{Suggestion ::: Improve Transfering}\n\n\\section{Suggestion ::: Discussion}\n\n\\section{Suggestion ::: Question}\n\n\\section{Suggestion ::: Answer}\n\n\\section{Suggestion ::: Suggestion}\n\n\\section{Suggestion", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "\n\n\\section{Introduction}\n\n\\textbf{Task}: The task is to retrieve the correct response of a target character by its HLAs.\n\n\\textbf{Human Impressions}: Human Impressions are the first step in the process of retrieving the correct response of a target character by its HLAs.\n\n\\textbf{Human Evaluation}: Human Evaluation is the second step in the process of retrieving the correct response of a target character by its HLAs.\n\n\\textbf{Baselines}: Baselines are the third step in the process of retrieving", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "\n\nAdversarial Reward Augmented Maximum Likelihood (ARAML) is a new adversarial training framework that aims to improve the stability of adversarial training. It is a combination of adversarial training and reward augmented maximum likelihood (Reward Augmented Maximum Likelihood).\n\nThe adversarial training is a technique that is used to improve the stability of adversarial training. It is a combination of adversarial training and reward augmented maximum likelihood (Reward Augmented Maximum Likelihood).\n\nThe adversarial training is a technique that is used", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nQuestion: What is the main reason for high misclassifications of hate samples as offensive?\n\nAnswer:[/INST]\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "\n\n### 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. ", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The dataset is of size 1000000.\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]The dataset is of size 1000000.\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]The dataset is of size 1000000.\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]The dataset is of size 1000000.\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]The dataset is of size 10", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "\n\n\\section{Introduction}\n\n\\textbf{Data Imbalance Issue in NLP}\n\nThe data imbalance issue is a common problem in NLP tasks. The majority of the datasets are composed of a large number of easy-positive examples and a small number of hard-negative examples. The easy-positive examples are the majority of the dataset, and the hard-negative examples are the minority of the dataset. The data imbalance issue is a severe problem for NLP tasks.\n\n\\textbf{Data Imbalance Issue in Object Detection}\n\nThe background-", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "\n\nThe datasets are the BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 ,", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "\nThe authors of the article describe the use of EEG signals to elicit event-related responses.\n\nQuestion: What is the purpose of the EEG signals?\n\nAnswer:[/INST]\nThe EEG signals are used to elicit event-related responses.\n\nQuestion: What is the purpose of the event-related responses?\n\nAnswer:[/INST]\nThe purpose of the event-related responses is to elucidate the neural mechanisms underlying the production of speech.\n\nQuestion: What is the purpose of the neural mechanisms?\n\nAnswer:[/INST]\n", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "\n\n UTF8gbsn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "\n\n\\section{Feature Engineering}\n\n\\textbf{Traditional Machine Learning Classifiers}\n\n\\textbf{Neural Network Models}\n\n\\textbf{Feature Extension}\n\n\\textbf{Dataset}\n\n\\textbf{Preprocessing}\n\n\\textbf{Training and Evaluation}\n\n\\textbf{Empirical Results}\n\n\\textbf{Discussion and Conclusion}\n\n\\textbf{Acknowledgments}\n\n\\section{Feature Engineering}\n\n\\textbf{Traditional Machine Learning Classifiers}\n\n\\textbf{Ne", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "\n\n\\section{Acknowledgements}\n\nWe thank the WMT'18 organizers for the great work and the WMT'18 dataset. We also thank the authors of BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "\n\n\\section{DSC Loss}\n\n\\begin{table}\n\\centering\n\\begin{tabular}{l}\n\\textbf{DSC Loss} \\\\\n\\hline\n\\textbf{Input:} \\\\\n$[h_1, p_1, h_2, p_2, ..., h_n, p_n]$ \\\\\n$h_i$ is the prediction of the model, $p_i$ is the probability of the positive class.\n\\end{tabular}\n\\end{table}\n\n\\begin{table}\n\\centering\n", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "\n\nAnswer:[/INST]\n\nAnswer:[/INST]\n\nAnswer:[/INST]\n\nAnswer:[/INST]\n\nAnswer:[/INST]\n\nAnswer:[/INST]\n\nAnswer:[/INST]\n\nAnswer:[/INST]\n\nAnswer:[/INST]\n\nAnswer:[/INST]\n\nAnswer:[/INST]\n\nAnswer:[/INST]\n\nAnswer:[/INST]\n\nAnswer:[/INST]\n\nAnswer:[/INST]\n\nAnswer:[/INST]\n\nAnswer:[/INST]\n\nAnswer:[/INST]\n\n", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "\nAn individual model consists of a monolingual model for each language and a multilingual model.\n\nQuestion: What is the monolingual model?\n\nAnswer:[/INST]\nThe monolingual model is a Bayesian model of semantic role induction.\n\nQuestion: What is the multilingual model?\n\nAnswer:[/INST]\nThe multilingual model uses word alignments between sentences in a parallel corpus to exploit role correspondences across languages.\n\nQuestion: What is the purpose of the crosslingual latent variables?\n\nAnswer:[/INST]", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "unanswerable\n\nQuestion: What is the purpose of the resource?\n\nAnswer:[/INST]unanswerable\n\nQuestion: What is the resource?\n\nAnswer:[/INST]unanswerable\n\nQuestion: What is the resource?\n\nAnswer:[/INST]unanswerable\n\nQuestion: What is the resource?\n\nAnswer:[/INST]unanswerable\n\nQuestion: What is the resource?\n\nAnswer:[/INST]unanswerable\n\nQuestion: What is the resource?\n\nAnswer:[/INST]unanswerable\n\nQuestion: What is the resource", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "\nA semicharacter architecture is a model that uses a character-level input and a word-level output.\n\nQuestion: What is the ScRNN model?\n\nAnswer:[/INST]\nThe ScRNN model is a semi-character word recognizer that uses a recurrent neural network to predict the correct spelling of a word.\n\nQuestion: What is the purpose of the ScRNN model?\n\nAnswer:[/INST]\nThe ScRNN model is used to correct spelling mistakes in a text.\n\nQuestion: What is the effect of adversarial attacks on the Sc", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "\n\n\\section{Introduction}\n\nThe introduction to the article is presented in a paragraph.\n\n\\section{Part-of-spece PoS tagging}\n\nThe PoS tagging task is to classify a word according to the part of speech.\n\n\\section{Morphological and lexical information}\n\nMorphological and lexical information is important for the task.\n\n\\section{External lexical information}\n\nExternal lexical information is important for the task.\n\n\\section{Word vector representations}\n\nWord vector representations are important for the task.\n", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "\n\n\\section{Introduction}\n\n\\textbf{Entity linking} is a fundamental problem in many fields, such as question answering BIBREF0 , semantic search BIBREF1 , and information extraction BIBREF2 , BIBREF3 . However, this task is non-trivial because entity mentions are usually ambiguous. As shown in Figure FIGREF1 , the mention England refers to three entities in KB, and an entity linking system should be capable of identifying the correct entity as England cricket team rather than England and England national football team.\n\n\\textbf{Global models}, which are", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes.\n\n### 1. Introduction\n\n### 2. Methods\n\n### 3. Results and Discussion\n\n### 4. Conclusion\n\n### 5. Acknowledgements\n\n### 6. References\n\n### 7. Appendix\n\n### 8. Citation\n\n### 9. Footnote\n\n### 10. Bibliography\n\n### 11. End-of-Document\n\n```python\n# %%\n# %%\n# %%\n# %%\n#", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "\n\nAnswer: The baseline is the FCE training set.\n\nAnswer: The baseline is the FCE training set.\n\nAnswer: The baseline is the FCE training set.\n\nAnswer: The baseline is the FCE training set.\n\nAnswer: The baseline is the FCE training set.\n\nAnswer: The baseline is the FCE training set.\n\nAnswer: The baseline is the FCE training set.\n\nAnswer: The baseline is the FCE training set.\n\nAnswer: The baseline is the FCE training set.\n\n", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "\nThe annotated clinical notes were obtained from the i2b2 dataset.\n\nQuestion: what is the i2b2 dataset?\n\nAnswer:[/INST]\nThe i2b2 dataset is a large dataset of clinical notes.\n\nQuestion: what is the purpose of the i2b2 dataset?\n\nAnswer:[/INST]\nThe i2b2 dataset is used for training the NER models.\n\nQuestion: what is the purpose of the NER models?\n\nAnswer:[/INST]\nThe NER models are used to extract clinical concepts from user queries", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "\n\nThe main reason is that the decoder is a neural network, and the neural network is a function that is not determinable. The decoder is a function that is not determinable, and the neural network is a function that is not determinable. The decoder is a function that is not determinable, and the neural network is a function that is not determinable. The decoder is a function that is not determinable, and the neural network is a function that is not determinable. The decoder is a function that is not determinable, and the neural network is a function that is not determinable.", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "\n\n### 2. Introduction\n\nThe paper is about the problem of learning unsupervised tweet representations.\n\n### 3. Unsupervised Tweet Representation Models\n\nMotivation: The motivation is to solve the problem of learning unsupervised tweet representations.\n\nModels: The models are the unsuperied twee representation models.\n\n### 4. Modeling within-tweet relationships\n\nMotivation: The motivation is to solve the problem of modeling within-tweet relationships.\n\nModels: The models are the un", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "TF-IDF features\n\nQuestion: What is the performance?\n\nAnswer:[/INST]The XGBoost classifier outperformed all other models for both the micro F-score metric, with a score of 0.92, and the macro F-score metric, with a score of 0.31.\n\nQuestion: What is the qualitative inspection?\n\nAnswer:[/INST]The proposed approach has performed well in highlighting the important regions, for example the topic highlighted with a red color provides useful biomarker information to readers.\n\nQuestion: What", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "\nThe dataset is annotated with 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression\") or evidence of depression (e.g., “depressed over disappointment\"). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., “feeling down in the dumps\").\n\nQuestion: What is the dataset?\n", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "\n\nThe authors evaluate on eight biomedical NER tasks.\n\nQuestion: What is the name of the repository?\n\nAnswer:[/INST]\n\nThe repository is called BioBERT.\n\nQuestion: What is the name of the paper?\n\nAnswer:[/INST]\n\nThe paper is called BioBERT: A Deeply-Elained Model for Biomedical NER.\n\nQuestion: What is the name of the dataset?\n\nAnswer:[/INST]\n\nThe dataset is called PubMed.\n\nQuestion: What is the name of the dataset?\n", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated by using the Apertium platform.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "\n\nThe model that the researchers used for their system is a content-based classifier.\n\nQuestion: What is the purpose of the system?\n\nAnswer:[/INST]\n\nThe purpose of the system is to predict a user's industry.\n\nQuestion: What is the dataset used in this study?\n\nAnswer:[/INST]\n\nThe dataset used in this study is a large, industry-annotated dataset containing over 20,000 blog users.\n\nQuestion: What is the examination of the content-based classifier?\n\nAnswer:[/INST]", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The baseline is the average of the performance of the 12 participants on the development set.\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]The dataset is a corpus of news articles annotated with 18 propaganda techniques at the fragment level.\n\nQuestion: What are the techniques?\n\nAnswer:[/INST]The 18 propaganda techniques are:\n\n\\begin{code}\n1. Loaded language.\n2. Name calling or labeling.\n3. Repetition.\n4. Exaggeration or minimization.\n5. Dou", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "\nUnanswerable.\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]\nThe dataset is the pun corpus.\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]\nThe dataset is the pun corpus.\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]\nThe dataset is the pun corpus.\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]\nThe dataset is the pun corpus.\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]\nThe dataset is the pun cor", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "The political bias of different sources is included in the model by using the dataset from BIBREF2. The dataset contains the information of the political bias of different sources.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer:[/INST] The two datasets are different in the size of the dataset. The US dataset is a larger dataset than the Italian dataset.\n\nQuestion: What is the difference between the two countries?\n\nAnswer:[/INST] The two countries are different in the political bias of the sources. The US dataset contains the information of the political bias of different sources. The Italian dataset", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The dataset comes from the ancient-modern Chinese translation task.\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]The dataset is a large-scale parallel corpus which contains INLINEREF1 1.24M bilingual sentence pairs.\n\nQuestion: What is the dataset size?\n\nAnswer:[/INST]The dataset size is INLINEREF2 1.24M bilingual sentence pairs.\n\nQuestion: What is the dataset size?\n\nAnswer:[/INST]The dataset size is INLINEREF3 1.24M bilingual", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English\n\nQuestion: What is the offensive content?\n\nAnswer:[/INST]Profane language\n\nQuestion: What is the target of the offensive content?\n\nAnswer:[/INST]Individual\n\nQuestion: What is the type of the offensive content?\n\nAnswer:[/INST]Threat\n\nQuestion: What is the target of the offensive content?\n\nAnswer:[/INST]Group\n\nQuestion: What is the type of the offensive content?\n\nAnswer:[/INST]Insult\n\nQuestion: What is the target of the offensive", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "\n\nThe answer is \"unanswerable\" because the article does not provide any information about the datasets.\n\nAnswer: which chinese datasets were used?\n\nAnswer:[/INST]\n\nThe answer is \"unanswerable\" because the article does not provide any information about the datasets.\n\nAnswer: which chinese datasets were used?\n\nAnswer:[/INST]\n\nThe answer is \"unanswerable\" because the article does not provide any information about the datasets.\n\nAnswer: which chinese datasets were used?\n\nAnswer:[/INST]\n\nThe answer is \"unanswer", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "3\n\nQuestion: What is the ultimate goal of the UTCNN model?\n\nAnswer:[/INST] To predict the stance of a post\n\nQuestion: What is the main difference between the UTCNN model and the ILP model?\n\nAnswer:[/INST] The UTCNN model is a neural network model that generates user embeddings for all users with minimum active degree, i.e., one post or one like. The ILP model is a model that integrates text features, author, ideology, and user-interaction constraints.\n\nQuestion: What is the main difference between", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "\nThe Flickr dataset is a large collection of photos from Flickr. The dataset is a collection of 1000000 images from Flickr. The dataset contains 1000000 images from Flickr.\n\nQuestion: what is the dataset used in this paper?\n\nAnswer:[/INST]\nThe Flickr dataset is a large collection of photos from Flickr. The dataset is a collection of 1000000 images from Flickr. The dataset contains 1000000 images from Flickr.", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "NUBes-PHI and MEDDOCAN.\n\nQuestion: What is the dataset MEDDOCAN?\n\nAnswer:[/INST]The MEDDOCAN dataset is a corpus of medical documents in English and Spanish. The dataset is composed of 21,371 English and 2,000 Spanish documents. The English documents are a mixture of medical reports and medical-related news. The Spanish documents are medical reports.\n\nQuestion: What is the dataset NUBes-PHI?\n\nAnswer:[/INST]The NUBes-PHI dataset is a cor", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "Unigram features, sarcasm features, readability features, word count features.\n\nQuestion: What is the improvement in the classification accuracy?\n\nAnswer:[/INST]The improvement in the classification accuracy is 3.7% and Kappa difference of 0.08.\n\nQuestion: What is the improvement in the classification accuracy with respect to the best available systems?\n\nAnswer:[/INST]The improvement in the classification accuracy is 2% with respect to the best available systems.\n\nQuestion: What is the improvement in the classification accuracy with respect to the best available systems with the help", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "\n\nThe metrics are the performance of the chatbots in the conversations. The chatbots are more knowledgeable and better at learning and conversations because the knowledge learning engine is able to learn new knowledge in the conversations.\n\nQuestion: What is the difference between the two approaches?\n\nAnswer:[/INST]\n\nThe two approaches are different in the way they solve the problem. The first approach is a closed-world approach, which assumes that the knowledge is known and the knowledge learning engine is able to solve the problem. The second approach is an open-world approach, which assumes that the knowledge is", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "\n\nTarget-1: Galatasaray\n\nTarget-2: Fenerbahçe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "\n\n\\section{Introduction}\n\nIrony is a kind of figurative language, which is widely used on social media. The irony is defined as a clash between the intended meaning of a sentence and its literal meaning. As an important aspect of language, irony plays an essential role in sentiment analysis, opinion mining, and other tasks.\n\nAlthough some previous studies focus on irony detection, little attention is paid to irony generation. As ironies can strengthen sentiments and express stronger emotions, we mainly focus on generating ironic sentences. Given a non-ironic sentence, we", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention is a variant of standard multi-head attention. The standard multi-head attention is a self-attention mechanism that is used to generate the representation of sentences. The standard multi-head attention is similar as Gaussian-masked direction attention but it does not have directional mask and gaussian mask. Gaussian-masked directional multi-head attention is a self-attention mechanism that is used to generate the representation of sentences. The Gaussian-masked directional multi-head attention is a variant of standard multi-head attention. The Gaussian-masked directional multi", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "The baseline features are the features that are extracted from the CNN architecture.\n\nQuestion: What are the pre-trained features?\n\nAnswer:[/INST]The pre-trained features are the features that are extracted from the pre-trained sentiment, emotion and personality models.\n\nQuestion: What is the F1-score?\n\nAnswer:[/INST]The F1-score is the average of the precision and the recall.\n\nQuestion: What is the F1-score of the baseline features?\n\nAnswer:[/INST]The F1-score of the bas", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "\n\\textbf{Answer 1:}\\textbf{Answer 2:}\\textbf{Answer 3:}\\textbf{Answer 4:}\\textbf{Answer 5:}\\textbf{Answer 6:}\\textbf{Answer 7:}\\textbf{Answer 8:}\\textbf{Answer 9:}\\textbf{Answer 10:}\\textbf{Answer 11:}\\textbf{Answer 12:}\\textbf{Answer 13:}\\textbf{Answer 14:}\\textbf{Answer 15:}\\textbf{Answer 16:}\\textbf{Answer 17:", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "\n\nAnswer: The scores of our system are as follows:\n\n\\begin{code}\nEI-Reg: 0.716\nEI-Oc: 0.716\nV-Reg: 0.716\nV-Oc: 0.716\n\\end{code}\n\n\\end{code}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus is a corpus of medical case reports. The corpus is a corpus of medical case reports.\n\nQuestion: What is the corpus?\n\nAnswer:[/INST] The corpus is a corpus of medical case reports. The corpus is a corpus of medical case reports.\n\nQuestion: What is the corpus?\n\nAnswer:[/INST] The corpus is a corpus of medical case reports. The corpus is a corpus of medical case reports.\n\nQuestion: What is the corpus?\n\nAnswer:[/INST] The corpus is a", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Yes.\n\nQuestion: What is the best way to pre-train a QA model?\n\nAnswer:[/INST] Pretrain on a large unlabeled dataset.\n\nQuestion: What is the best way to pre-train a QA model?\n\nAnswer:[/INST] Pretrain on a large unlabeled dataset.\n\nQuestion: What is the best way to pre-train a QA model?\n\nAnswer:[/INST] Pretrain on a large unlabeled dataset.\n\nQuestion: What is the best way to pre-train a QA model?", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "\n\nAnswer: Baseball-Ho0.\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]\n\nAnswer: The dataset is the movie dataset.\n\nQuestion: What is the labeled features?\n\nAnswer:[/INST]\n\nAnswer: The labeled features are the features that are predictive of the class.\n\nQuestion: What is the model?\n\nAnswer:[/INST]\n\nAnswer: The model is the GE-FL model.\n\nQuestion: What is the regularization?\n\nAnswer:[/INST]\n\nAnswer: The regularization", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "\n\nAnswer: The model is compared to the TREC-based methods of Li and Roth BIBREF6 .\n\nAnswer:[/INST]\n\nAnswer: The model is compared to the TREC-based methods of Li and Roth BIBREF6 .\n\nAnswer:[/INST]\n\nAnswer: The model is compared to the TREC-based methods of Li and Roth BIBREF6 .\n\nAnswer:[/INST]\n\nAnswer: The model is compared to the TREC-based methods of Li and Roth BIBREF6 .\n\nAnswer:[/INST]", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "The training sets are larger than the previous ones.\n\nQuestion: What is the size of the training sets?\n\nAnswer:[/INST] The training sets are larger than the previous ones.\n\nQuestion: What is the size of the training sets?\n\nAnswer:[/INST] The training sets are larger than the previous ones.\n\nQuestion: What is the size of the training sets?\n\nAnswer:[/INST] The training sets are larger than the previous ones.\n\nQuestion: What is the size of the training sets?\n\nAnswer:[/INST] The training sets are larger than the previous", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "1000\n\nQuestion: What is the dataset size?\n\nAnswer:[/INST] 1000\n\nQuestion: What is the dataset size?\n\nAnswer:[/INST] 1000\n\nQuestion: What is the dataset size?\n\nAnswer:[/INST] 1000\n\nQuestion: What is the dataset size?\n\nAnswer:[/INST] 1000\n\nQuestion: What is the dataset size?\n\nAnswer:[/INST] 1000\n\nQuestion: What is the dataset size?\n\nAnswer", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]\n\nQuestion: What is the task?\n\nAnswer:[/INST]\n\nQuestion: What is the training data?\n\nAnswer:[/INST]\n\nQuestion: What is the test data?\n\nAnswer:[/INST]\n\nQuestion: What is the model?\n\nAnswer:[/INST]\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]\n\nQuestion: What is the task?\n\nAnswer:[/INST]\n\nQuestion: What is the training data?\n\nAnswer:[", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes.\n\nQuestion: What is the model's architecture?\n\nAnswer:[/INST]The model's architecture is a combination of LSTM and CNN.\n\nQuestion: What is the model's modality attention?\n\nAnswer:[/INST]The model's modality attention is a neural mechanism that learns to attenuate irrelevant or uninformative modal information while amplifying the primary modality to extract better overall representations.\n\nQuestion: How does the model's modality attention improve the model's performance?\n\nAnswer:[/INST]The model's mod", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "Yes.\n\nQuestion: What is the dataset?\n\nAnswer:[/INST]WSJ.\n\nQuestion: What is the model?\n\nAnswer:[/INST]Gaussian HMM.\n\nQuestion: What is the model?\n\nAnswer:[/INST]Markov-structured.\n\nQuestion: What is the model?\n\nAnswer:[/INST]DMV.\n\nQuestion: What is the model?\n\nAnswer:[/INST]Random initialization.\n\nQuestion: What is the model?\n\nAnswer:[/INST]FastText.\n\nQuestion: What is", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "\n\n\\section{Exact Answer Generation}\n\n\\section{Factoid Question Answering}\n\n\\section{List-type Question Answering}\n\n\\section{Summary}\n\n\\section{Acknowledgements}\n\n\\section{References}\n\n\\begin{code}\n<ref>\n\nBibRef{1}\n\nBert-Large: An Updated Large-Scale BERT Model for\n\nExtremely\n\nLanin, A., and I. Gorashchok, 2019.\n\nBert-", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "\n\n\\section{Introduction}\n\n\\textbf{Unpaired image-to-image translation} is the task of mapping an image in one domain to an image in another domain. The goal is to produce an image that is visually similar to the input image in the target domain.\n\n\\textbf{Image-to-image translation} is a challenging problem due to the large amount of data required to learn a mapping from one domain to another. In addition, the two domains are often different in terms of the visual appearance, the content, and the style.\n\n\\textbf{Unpaired image-", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "\n\nThe above answer is unanswerable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "\n\nOn WebQSP, we achieve 82.53% top-1 accuracy. On SimpleQuestions, we achieve 88.8% top-1 accuracy.\n\nQuestion: What is the improvement on WebQSP?\n\nAnswer:[/INST]\n\nThe improvement on WebQSP is 2.5% top-1.\n\nQuestion: What is the improvement on SimpleQuestions?\n\nAnswer:[/INST]\n\nThe improvement on SimpleQuestions is 1.2% top-1.\n\nQuestion: What is the improvement on STAGG?", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
